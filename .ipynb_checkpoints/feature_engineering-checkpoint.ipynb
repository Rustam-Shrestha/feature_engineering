{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9a60b49",
   "metadata": {},
   "source": [
    "# Feature Engineering with Pandas\n",
    "\n",
    "This notebook covers various aspects of handling data in Pandas, including types of data, selecting data types, dealing with categorical variables, encoding, handling uncommon categories, numeric variables, binarizing, binning, and handling missing data.\n",
    "\n",
    "We will use real datasets:\n",
    "- Stack Overflow Developer Survey 2023: https://raw.githubusercontent.com/Stephen137/stack_overflow_developer_survey_2023/main/data/survey_results_public_2023.csv\n",
    "- NYC Restaurant Inspection Results: https://data.cityofnewyork.us/api/views/43nn-pn8j/rows.csv?accessType=DOWNLOAD\n",
    "\n",
    "Note: The Stack Overflow data has columns like 'Country', 'ConvertedCompYearly' (similar to ConvertedSalary), etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e8fab8",
   "metadata": {},
   "source": [
    "## Types of Data\n",
    "\n",
    "- Continuous data\n",
    "- Categorical (e.g., gender, birth country)\n",
    "- Ordinal: order without actual distance\n",
    "- Boolean\n",
    "- Date time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254f8398",
   "metadata": {},
   "source": [
    "## Exercise 1: Loading Data and Checking Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ae3f376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      qid       qname                                           question  \\\n",
      "0    QID2  MainBranch  Which of the following options best describes ...   \n",
      "1  QID127         Age                                 What is your age?*   \n",
      "2  QID296  Employment  Which of the following best describes your cur...   \n",
      "3  QID308  RemoteWork  Which best describes your current work situation?   \n",
      "4  QID341       Check  Just checking to make sure you are paying atte...   \n",
      "\n",
      "  force_resp type selector  \n",
      "0       True   MC     SAVR  \n",
      "1       True   MC     SAVR  \n",
      "2       True   MC     MAVR  \n",
      "3      False   MC     SAVR  \n",
      "4       True   MC     SAVR  \n",
      "\n",
      "Column Data Types:\n",
      "qid           object\n",
      "qname         object\n",
      "question      object\n",
      "force_resp    object\n",
      "type          object\n",
      "selector      object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the URL for Stack Overflow survey\n",
    "so_survey_csv = \"survey_results_schema.csv\"\n",
    "# Load the data\n",
    "so_survey_df = pd.read_csv(so_survey_csv)\n",
    "\n",
    "# Print the first five rows\n",
    "print(so_survey_df.head())\n",
    "\n",
    "# Print the data types\n",
    "print('\\nColumn Data Types:')\n",
    "print(so_survey_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d1e192",
   "metadata": {},
   "source": [
    "## Selecting Specific Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a2073f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Create subset of only the numeric columns\n",
    "so_numeric_df = so_survey_df.select_dtypes(include=['int', 'float'])\n",
    "\n",
    "# Print the column names\n",
    "print(so_numeric_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7e3960",
   "metadata": {},
   "source": [
    "## Dealing with Categorical Variables\n",
    "\n",
    "We encode categorical variables into numbers or booleans.\n",
    "\n",
    "Types of encoding:\n",
    "1. One-Hot Encoding: n categories into n features.\n",
    "2. Dummy Encoding: n categories into n-1 features, omitting one to avoid collinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b01fbf",
   "metadata": {},
   "source": [
    "## One-Hot Encoding Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "286c3889",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Name', 'OH_Germany', 'OH_India', 'OH_Nepal', 'OH_USA'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a sample dataset\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eva', 'Frank', 'Grace', 'Hannah', 'Ian', 'Jack',\n",
    "             'Kira', 'Liam', 'Mona', 'Nina', 'Oscar', 'Paul', 'Quinn', 'Rita', 'Sam', 'Tina'],\n",
    "    'Country': ['USA', 'India', 'USA', 'Germany', 'India', 'Nepal', 'Germany', 'USA', 'Nepal', 'India',\n",
    "                'Germany', 'USA', 'Nepal', 'India', 'USA', 'Germany', 'Nepal', 'India', 'USA', 'Nepal']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the dataset\n",
    "# print(df)\n",
    "\n",
    "\n",
    "# Convert the Country column to one-hot encoded DataFrame\n",
    "one_hot_encoded = pd.get_dummies(df, columns=['Country'], prefix='OH')\n",
    "\n",
    "# Print the column names\n",
    "print(one_hot_encoded.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd332cf7",
   "metadata": {},
   "source": [
    "## Dummy Encoding Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afe4d838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Name', 'DM_India', 'DM_Nepal', 'DM_USA'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Create dummy variables for the Country column\n",
    "dummy = pd.get_dummies(df, columns=['Country'], drop_first=True, prefix='DM')\n",
    "\n",
    "# Print the column names\n",
    "print(dummy.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319a0335",
   "metadata": {},
   "source": [
    "## Dealing with Uncommon Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94edeafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country\n",
      "Other    20\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create a series out of the Country column\n",
    "countries = df['Country']\n",
    "\n",
    "# Get the counts of each category\n",
    "country_counts = countries.value_counts()\n",
    "\n",
    "# Create a mask for categories that occur less than 10 times\n",
    "mask = countries.isin(country_counts[country_counts < 10].index)\n",
    "\n",
    "# Label all other categories as 'Other'\n",
    "df.loc[mask, 'Country'] = 'Other'\n",
    "\n",
    "# Print the updated category counts\n",
    "print(df['Country'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a84efd",
   "metadata": {},
   "source": [
    "## Numeric Variables\n",
    "\n",
    "Example with Restaurant Data: Binarizing violations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df58a91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      CAMIS                  DBA       BORO BUILDING             STREET  \\\n",
      "0  50175073      KUCHELA KUIZINE   Brooklyn     1197    FLATBUSH AVENUE   \n",
      "1  50170738  787 COFFEE WEST LLC  Manhattan      245   WEST   46 STREET   \n",
      "2  50036660          KIKOO SUSHI  Manhattan      141           1 AVENUE   \n",
      "3  50172620           FRIJOLEROS   Brooklyn      131  GREENPOINT AVENUE   \n",
      "4  50171814     ACE SHAWARMA INC      Bronx     3455      JEROME AVENUE   \n",
      "\n",
      "   ZIPCODE       PHONE CUISINE DESCRIPTION INSPECTION DATE  \\\n",
      "0  11226.0  3473355072                 NaN      01/01/1900   \n",
      "1  10036.0  9082308846                 NaN      01/01/1900   \n",
      "2  10003.0  2125333888            Japanese      03/25/2024   \n",
      "3  11222.0  3473842957                 NaN      01/01/1900   \n",
      "4  10467.0  6467023905                 NaN      01/01/1900   \n",
      "\n",
      "                                            ACTION  ...  \\\n",
      "0                                              NaN  ...   \n",
      "1                                              NaN  ...   \n",
      "2  Violations were cited in the following area(s).  ...   \n",
      "3                                              NaN  ...   \n",
      "4                                              NaN  ...   \n",
      "\n",
      "                         INSPECTION TYPE   Latitude  Longitude  \\\n",
      "0                                    NaN  40.641133 -73.956249   \n",
      "1                                    NaN  40.759204 -73.986536   \n",
      "2  Cycle Inspection / Initial Inspection  40.727952 -73.985034   \n",
      "3                                    NaN  40.730126 -73.955025   \n",
      "4                                    NaN  40.881555 -73.882581   \n",
      "\n",
      "   Community Board Council District Census Tract        BIN           BBL  \\\n",
      "0            314.0             45.0      79000.0  3119633.0  3.051880e+09   \n",
      "1            105.0              3.0      12500.0  1024737.0  1.010180e+09   \n",
      "2            103.0              2.0       3800.0  1006381.0  1.004500e+09   \n",
      "3            301.0             33.0      56500.0  3064721.0  3.025580e+09   \n",
      "4            207.0             11.0      42100.0  2017713.0  2.033240e+09   \n",
      "\n",
      "    NTA  Location Point1  \n",
      "0  BK95              NaN  \n",
      "1  MN17              NaN  \n",
      "2  MN22              NaN  \n",
      "3  BK76              NaN  \n",
      "4  BX43              NaN  \n",
      "\n",
      "[5 rows x 27 columns]\n",
      "   SCORE  has_violation\n",
      "0    NaN              0\n",
      "1    NaN              0\n",
      "2   36.0              1\n",
      "3    NaN              0\n",
      "4    NaN              0\n"
     ]
    }
   ],
   "source": [
    "# Load NYC Restaurant Inspection Data\n",
    "restaurant_csv = 'https://data.cityofnewyork.us/api/views/43nn-pn8j/rows.csv?accessType=DOWNLOAD'\n",
    "restaurant_df = pd.read_csv(restaurant_csv)\n",
    "\n",
    "# Print head\n",
    "print(restaurant_df.head())\n",
    "\n",
    "# For simplicity, assume 'SCORE' represents violation score (higher score = more violations)\n",
    "# Create a binary column: has_violation if SCORE > 0\n",
    "restaurant_df['has_violation'] = 0\n",
    "restaurant_df.loc[restaurant_df['SCORE'] > 0, 'has_violation'] = 1\n",
    "\n",
    "# Print sample\n",
    "print(restaurant_df[['SCORE', 'has_violation']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e1616e",
   "metadata": {},
   "source": [
    "## Binning Numeric Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a945f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Back to SO data for salary binning\n",
    "# Note: Column is 'ConvertedCompYearly'\n",
    "\n",
    "# Create Paid_Job column filled with zeros\n",
    "so_survey_df['Paid_Job'] = 0\n",
    "\n",
    "# Replace where ConvertedCompYearly > 0\n",
    "so_survey_df.loc[so_survey_df['ConvertedCompYearly'] > 0, 'Paid_Job'] = 1\n",
    "\n",
    "# Print sample\n",
    "print(so_survey_df[['Paid_Job', 'ConvertedCompYearly']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101704c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Specify bin boundaries\n",
    "bins = [-np.inf, 10000, 50000, 100000, 150000, np.inf]\n",
    "\n",
    "# Bin labels\n",
    "labels = ['Very low', 'Low', 'Medium', 'High', 'Very high']\n",
    "\n",
    "# Bin the ConvertedCompYearly\n",
    "so_survey_df['boundary_binned'] = pd.cut(so_survey_df['ConvertedCompYearly'], bins=bins, labels=labels)\n",
    "\n",
    "# Print sample\n",
    "print(so_survey_df[['boundary_binned', 'ConvertedCompYearly']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb44617",
   "metadata": {},
   "source": [
    "# Day 2\n",
    "## Handling Gaps in Data (Missing Values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f652d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check info\n",
    "so_survey_df.info()\n",
    "\n",
    "# Check missing values\n",
    "print(so_survey_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6229492c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For restaurant data\n",
    "restaurant_df.info()\n",
    "\n",
    "print(restaurant_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908d3db4-a2a8-4ee2-a36a-7c1cd58b4922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data info and null values\n",
    "print(so_survey_df.info())\n",
    "print(so_survey_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667fcf6b-ff40-4882-8099-02152950c263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the DataFrame and print the number of non-missing values\n",
    "sub_df = so_survey_df[['Age', 'Gender', 'ConvertedSalary']]\n",
    "print(sub_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314ccba7-eddf-4d5d-bd30-b35f50ba343e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the top 10 entries of the DataFrame\n",
    "print(sub_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2c8d9e-0a2a-46b4-8790-6e87c7f580dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the locations of the missing values\n",
    "print(sub_df.head(10).isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d08e56-f7a6-4ccf-921f-b5d02cc9b0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the locations of the non-missing values\n",
    "print(sub_df.head(10).notnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98176bca-edaa-4d4d-9ff1-bd856490dba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of rows and columns\n",
    "print(so_survey_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9fcff4-f9e0-4fda-b960-653832bbf45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame dropping all incomplete rows\n",
    "no_missing_values_rows = so_survey_df.dropna(how='any')\n",
    "print(no_missing_values_rows.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e1a871-cdd9-4061-ab3d-54d5f7308a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame dropping all columns with incomplete rows\n",
    "no_missing_values_cols = so_survey_df.dropna(how='any', axis=1)\n",
    "print(no_missing_values_cols.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef6daae-c680-47a9-b250-38ffbf4f4d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all rows where Gender is missing\n",
    "no_gender = so_survey_df.dropna(subset=['Gender'])\n",
    "print(no_gender.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc27599d-020f-4186-bb21-bc119b690c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing values in Gender with 'Not Given'\n",
    "so_survey_df['Gender'].fillna(value='Not Given', inplace=True)\n",
    "print(so_survey_df['Gender'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd988b57-da55-42d6-8336-88d64a352b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first five rows of StackOverflowJobsRecommend column\n",
    "print(so_survey_df['StackOverflowJobsRecommend'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d2f179-5c0b-4015-8aa1-0d07aa7c8851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values with the mean\n",
    "so_survey_df['StackOverflowJobsRecommend'].fillna(so_survey_df['StackOverflowJobsRecommend'].mean(), inplace=True)\n",
    "print(so_survey_df['StackOverflowJobsRecommend'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bbbcb7-fdff-4075-91f3-4e7a48d882a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values with the mean and round\n",
    "so_survey_df['StackOverflowJobsRecommend'].fillna(so_survey_df['StackOverflowJobsRecommend'].mean(), inplace=True)\n",
    "so_survey_df['StackOverflowJobsRecommend'] = round(so_survey_df['StackOverflowJobsRecommend'])\n",
    "print(so_survey_df['StackOverflowJobsRecommend'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69243de8-d43e-465c-8998-9562c880d792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove commas in the RawSalary column\n",
    "so_survey_df['RawSalary'] = so_survey_df['RawSalary'].str.replace(',', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21f2beb-b031-4d47-8259-19f0efbd35b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove dollar signs in the RawSalary column\n",
    "so_survey_df['RawSalary'] = so_survey_df['RawSalary'].str.replace('$', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16f75b2-dd9d-4c40-a8ea-30fd00750d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to convert RawSalary to numeric, coercing errors to NaN\n",
    "numeric_vals = pd.to_numeric(so_survey_df['RawSalary'], errors='coerce')\n",
    "idx = numeric_vals.isna()\n",
    "print(so_survey_df['RawSalary'][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd48834-0575-4e1e-99e6-02611785f9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace pound signs and convert RawSalary to float\n",
    "so_survey_df['RawSalary'] = so_survey_df['RawSalary'].str.replace('£', '')\n",
    "so_survey_df['RawSalary'] = so_survey_df['RawSalary'].astype('float')\n",
    "print(so_survey_df['RawSalary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21654446-317c-4d8e-8e81-2b1306a9679f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use method chaining to clean and convert RawSalary\n",
    "so_survey_df['RawSalary'] = so_survey_df['RawSalary']\\\n",
    "    .str.replace(',', '')\\\n",
    "    .str.replace('$', '')\\\n",
    "    .str.replace('£', '')\\\n",
    "    .astype('float')\n",
    "print(so_survey_df['RawSalary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad13267-727c-438a-9c69-37b1d422b001",
   "metadata": {},
   "source": [
    "# day 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdea6909-07d6-480e-a310-886dd56e31ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for visualization and preprocessing\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, PowerTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95c3643-a404-4f05-b770-c7cd5a7a0cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a histogram of all numeric columns\n",
    "# Visualizes the distribution of numerical features\n",
    "so_numeric_df.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce649630-fa43-4b16-a565-7e2f1aa01548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boxplot for Age and Years Experience\n",
    "# Helps identify the spread and potential outliers in these columns\n",
    "so_numeric_df[['Age', 'Years Experience']].boxplot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9d62c2-7594-4f30-a5f7-aed71346774f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot pairwise relationships for all numeric columns\n",
    "# Uses seaborn to show scatter plots and histograms for feature interactions\n",
    "sns.pairplot(so_numeric_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f9d11e-f8f7-411b-b6d7-22a94c625fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot pairwise relationships for all numeric columns\n",
    "# Uses seaborn to show scatter plots and histograms for feature interactions\n",
    "sns.pairplot(so_numeric_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7489a715-090a-4b65-a263-52aea62a9ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply MinMaxScaler to scale Age between 0 and 1\n",
    "# Ensures features are on the same scale for models sensitive to magnitude\n",
    "MM_scaler = MinMaxScaler()\n",
    "MM_scaler.fit(so_numeric_df[['Age']])\n",
    "so_numeric_df['Age_MM'] = MM_scaler.transform(so_numeric_df[['Age']])\n",
    "print(so_numeric_df[['Age_MM', 'Age']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9306d586-4295-44e4-b090-4bd7d5238857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply StandardScaler to standardize Age\n",
    "# Centers the data around mean with unit standard deviation\n",
    "SS_scaler = StandardScaler()\n",
    "SS_scaler.fit(so_numeric_df[['Age']])\n",
    "so_numeric_df['Age_SS'] = SS_scaler.transform(so_numeric_df[['Age']])\n",
    "print(so_numeric_df[['Age_SS', 'Age']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef2d4be-dd5c-4961-be77-f77c6d312ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PowerTransformer to reduce skewness in ConvertedSalary\n",
    "# Uses Box-Cox or Yeo-Johnson to make data more normally distributed\n",
    "pow_trans = PowerTransformer()\n",
    "pow_trans.fit(so_numeric_df[['ConvertedSalary']])\n",
    "so_numeric_df['ConvertedSalary_LG'] = pow_trans.transform(so_numeric_df[['ConvertedSalary']])\n",
    "so_numeric_df[['ConvertedSalary', 'ConvertedSalary_LG']].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48529978-0227-44ae-8876-b7c0167b41f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PowerTransformer to reduce skewness in ConvertedSalary\n",
    "# Uses Box-Cox or Yeo-Johnson to make data more normally distributed\n",
    "pow_trans = PowerTransformer()\n",
    "pow_trans.fit(so_numeric_df[['ConvertedSalary']])\n",
    "so_numeric_df['ConvertedSalary_LG'] = pow_trans.transform(so_numeric_df[['ConvertedSalary']])\n",
    "so_numeric_df[['ConvertedSalary', 'ConvertedSalary_LG']].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38145718-63db-4cd2-8a4e-8affa5b367e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers using the 95th quantile for ConvertedSalary\n",
    "# Trims the top 5% of data to reduce the impact of extreme values\n",
    "quantile = so_numeric_df['ConvertedSalary'].quantile(0.95)\n",
    "trimmed_df = so_numeric_df[so_numeric_df['ConvertedSalary'] < quantile]\n",
    "so_numeric_df[['ConvertedSalary']].hist()\n",
    "plt.show()\n",
    "plt.clf()\n",
    "trimmed_df[['ConvertedSalary']].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d87072-daef-4d71-b5ac-e5f67efae788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers using statistical method (3 standard deviations)\n",
    "# Trims data points outside 3 standard deviations from the mean\n",
    "std = so_numeric_df['ConvertedSalary'].std()\n",
    "mean = so_numeric_df['ConvertedSalary'].mean()\n",
    "cut_off = std * 3\n",
    "lower, upper = mean - cut_off, mean + cut_off\n",
    "trimmed_df = so_numeric_df[(so_numeric_df['ConvertedSalary'] < upper) & (so_numeric_df['ConvertedSalary'] > lower)]\n",
    "trimmed_df[['ConvertedSalary']].boxplot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfd45fb-adca-4b54-a001-f0cb095e3f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply StandardScaler to training data and transform test data\n",
    "# Ensures no data leakage by fitting scaler only on training data\n",
    "SS_scaler = StandardScaler()\n",
    "SS_scaler.fit(so_train_numeric[['Age']])\n",
    "so_test_numeric['Age_ss'] = SS_scaler.transform(so_test_numeric[['Age']])\n",
    "print(so_test_numeric[['Age', 'Age_ss']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa1b990-c1b8-4b4e-a147-6d8128a3b227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers in test data using training data thresholds\n",
    "# Uses mean and standard deviation from training data to avoid leakage\n",
    "train_std = so_train_numeric['ConvertedSalary'].std()\n",
    "train_mean = so_train_numeric['ConvertedSalary'].mean()\n",
    "cut_off = train_std * 3\n",
    "train_lower, train_upper = train_mean - cut_off, train_mean + cut_off\n",
    "trimmed_df = so_test_numeric[(so_test_numeric['ConvertedSalary'] < train_upper) & (so_test_numeric['ConvertedSalary'] > train_lower)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22166a9-6358-4309-b0ab-f47ffdaba966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers in test data using training data thresholds\n",
    "# Uses mean and standard deviation from training data to avoid leakage\n",
    "train_std = so_train_numeric['ConvertedSalary'].std()\n",
    "train_mean = so_train_numeric['ConvertedSalary'].mean()\n",
    "cut_off = train_std * 3\n",
    "train_lower, train_upper = train_mean - cut_off, train_mean + cut_off\n",
    "trimmed_df = so_test_numeric[(so_test_numeric['ConvertedSalary'] < train_upper) & (so_test_numeric['ConvertedSalary'] > train_lower)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28450a3-fd59-4a89-a55f-78f34ff8eca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first 5 rows of the text column\n",
    "# Displays raw text data for inspection\n",
    "print(speech_df['text'].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc32eb74-8d63-4f49-9946-14429344045d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text by removing non-letters and converting to lowercase\n",
    "# Prepares text for further processing by standardizing it\n",
    "speech_df['text_clean'] = speech_df['text'].str.replace('[^a-zA-Z]', ' ')\n",
    "speech_df['text_clean'] = speech_df['text_clean'].str.lower()\n",
    "print(speech_df['text_clean'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ab46f5-b98d-4946-ad0c-45351eb104db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract high-level text features\n",
    "# Calculates character count, word count, and average word length\n",
    "speech_df['char_cnt'] = speech_df['text_clean'].str.len()\n",
    "speech_df['word_cnt'] = speech_df['text_clean'].str.split().str.len()\n",
    "speech_df['avg_word_length'] = speech_df['char_cnt'] / speech_df['word_cnt']\n",
    "print(speech_df[['text_clean', 'char_cnt', 'word_cnt', 'avg_word_length']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5f7a3d-1790-4033-8160-2dcfb69bd5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count words using CountVectorizer\n",
    "# Creates a sparse matrix of word counts\n",
    "cv = CountVectorizer()\n",
    "cv.fit(speech_df['text_clean'])\n",
    "print(cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b827311b-2fb2-4594-90bf-7808425e680a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform text to word count array\n",
    "# Converts text to a matrix of word frequencies\n",
    "cv_transformed = cv.transform(speech_df['text_clean'])\n",
    "cv_array = cv_transformed.toarray()\n",
    "print(cv_array)\n",
    "print(cv_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d617fd-69ae-481c-a23d-17fef4ffcc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit features with min_df and max_df\n",
    "# Reduces features by keeping words in 20-80% of documents\n",
    "cv = CountVectorizer(min_df=0.2, max_df=0.8)\n",
    "cv_transformed = cv.fit_transform(speech_df['text_clean'])\n",
    "cv_array = cv_transformed.toarray()\n",
    "print(cv_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b87944c-e51e-4c2f-ab0f-4448a53d37c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame from word counts\n",
    "# Adds word count features to the original DataFrame\n",
    "cv_df = pd.DataFrame(cv_array, columns=cv.get_feature_names_out()).add_prefix('Counts_')\n",
    "speech_df_new = pd.concat([speech_df, cv_df], axis=1, sort=False)\n",
    "print(speech_df_new.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0c061a-1d31-4d13-9dbb-bc9dbc3c2694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply TfidfVectorizer with feature limits and stop words\n",
    "# Creates TF-IDF features, reducing the impact of common words\n",
    "tv = TfidfVectorizer(max_features=100, stop_words='english')\n",
    "tv_transformed = tv.fit_transform(speech_df['text_clean'])\n",
    "tv_df = pd.DataFrame(tv_transformed.toarray(), columns=tv.get_feature_names_out()).add_prefix('TFIDF_')\n",
    "print(tv_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f441bd-4e0d-41ba-82c4-eedb50ca559b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect top 5 TF-IDF words in the first document\n",
    "# Identifies the most important words in a single document\n",
    "sample_row = tv_df.iloc[0]\n",
    "print(sample_row.sort_values(ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47824413-5600-4303-8ae3-cd6a569e3af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform test data using fitted TfidfVectorizer\n",
    "# Applies the same transformation to avoid data leakage\n",
    "tv = TfidfVectorizer(max_features=100, stop_words='english')\n",
    "tv_transformed = tv.fit_transform(train_speech_df['text_clean'])\n",
    "test_tv_transformed = tv.transform(test_speech_df['text_clean'])\n",
    "test_tv_df = pd.DataFrame(test_tv_transformed.toarray(), columns=tv.get_feature_names_out()).add_prefix('TFIDF_')\n",
    "print(test_tv_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5747e120-1437-4ebc-aa5e-ab795650e852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trigram features using CountVectorizer\n",
    "# Extracts three-word phrases to capture context\n",
    "cv_trigram_vec = CountVectorizer(max_features=100, stop_words='english', ngram_range=(3, 3))\n",
    "cv_trigram = cv_trigram_vec.fit_transform(speech_df['text_clean'])\n",
    "print(cv_trigram_vec.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6b1dad-2eb9-42c5-b970-d310020d8010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the most common trigrams\n",
    "# Identifies frequently occurring three-word phrases\n",
    "cv_tri_df = pd.DataFrame(cv_trigram.toarray(), columns=cv_trigram_vec.get_feature_names_out()).add_prefix('Counts_')\n",
    "print(cv_tri_df.sum().sort_values(ascending=False).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b93122-1173-4043-9628-86e58a0127d0",
   "metadata": {},
   "source": [
    "# THE END \n",
    "## TO BE CONTINUED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7337ce-9ba1-400a-9068-1661d4d85743",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec82e74c-400d-440b-b4a5-c61494e1bd31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b35874-ddaa-4e59-9bea-b4f66224639a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952c98c6-caeb-4a82-883a-c8b6342be257",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8448b9d-012c-47b4-ab8a-d2da5d2bbfbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
